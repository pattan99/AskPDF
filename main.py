import streamlit as st
from streamlit_chat import message

from backend.core import run_llm
from backend.store_pdf import store_pdf

# Set layout to wide mode
st.set_page_config(layout="wide")

if "user_prompt_history" not in st.session_state:
    st.session_state["user_prompt_history"] = []
if "chat_answers_history" not in st.session_state:
    st.session_state["chat_answers_history"] = []
if "load_pdf" not in st.session_state:
    st.session_state["load_pdf"] = False

# Sidebar
with st.sidebar:
    st.title("Project Description")
    st.write("This project allows users to interactively chat with a language model based on prompts. Users can upload a PDF file, ask questions related to the content of the PDF, and receive responses generated by the language model.")
    st.write("The chat history is displayed in real-time, with the user's prompt followed by the model's response.")

    uploaded_file = st.file_uploader("Upload your PDF file", type="pdf")
    if uploaded_file is not None and not st.session_state["load_pdf"]:
        bytes_data = uploaded_file.read()

        with open("uploaded_file.pdf", "wb") as f:
            f.write(bytes_data)

        with st.spinner("Saving pdf into database..."):
            store_pdf()

        st.session_state["load_pdf"] = True
        st.success("Saved successfully!")

    st.markdown("[GitHub](https://github.com/pattan99)")
    st.markdown("[LinkedIn](https://www.linkedin.com/in/toanhphat/)")

# Main content
with st.container():
    st.title("Ask on PDF")

    prompt = st.text_input("Prompt", placeholder="Enter your prompt here...")

    if prompt:
        with st.spinner("Generating response..."):
            generated_response = run_llm(query=prompt)
            formatted_response = generated_response['result']

            st.session_state["user_prompt_history"].append(prompt)
            st.session_state["chat_answers_history"].append(formatted_response)

    if st.session_state["chat_answers_history"]:
        for generated_response, user_query in zip(st.session_state["chat_answers_history"], st.session_state["user_prompt_history"]):
            message(user_query, is_user=True)
            message(generated_response)
